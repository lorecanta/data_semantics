{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model_and_files, get_model_details, reconstruct_word, merge_results, preprocess_text, preprocess_stopwords, preprocess_lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carico i modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "osiria/bert-italian-uncased-ner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVN-RIN/MedPsyNIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SamLowe/roberta-base-go_emotions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zaneas/Traduttore_IT_EN_2\n",
      "Errore nel caricamento del modello zaneas/Traduttore_IT_EN_2: Unrecognized configuration class <class 'transformers.models.marian.configuration_marian.MarianConfig'> for this kind of AutoModel: AutoModelForTokenClassification.\n",
      "Model type should be one of AlbertConfig, BertConfig, BigBirdConfig, BioGptConfig, BloomConfig, BrosConfig, CamembertConfig, CanineConfig, ConvBertConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GemmaConfig, Gemma2Config, GlmConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MobileBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, NemotronConfig, NezhaConfig, NystromformerConfig, PersimmonConfig, PhiConfig, Phi3Config, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, StableLmConfig, Starcoder2Config, T5Config, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.\n"
     ]
    }
   ],
   "source": [
    "# 1. Recupera gli ID dei modelli e i relativi file dal file .env\n",
    "(model_1_id, model_1_files), (model_2_id, model_2_files), (model_3_id, model_3_files), (model_4_id, model_4_files) = get_model_details()\n",
    "\n",
    "# 2. Carica le pipeline per i modelli\n",
    "print(model_1_id)\n",
    "model_1_pipeline = load_model_and_files(model_1_id, model_1_files)\n",
    "\n",
    "print(model_2_id)\n",
    "model_2_pipeline = load_model_and_files(model_2_id, model_2_files)\n",
    "\n",
    "print(model_3_id)\n",
    "model_3_pipeline = load_model_and_files(model_3_id, model_3_files)\n",
    "\n",
    "print(model_4_id)\n",
    "model_4_pipeline = load_model_and_files(model_4_id, model_4_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giuly\\Documents\\GitHub\\data_semantics\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_4_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_4_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faccio una prova utilizzando direttamente i modelli sul testo crudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorrei sapere se Giulia che vive Milano c'è qualche (seria)speranza anche per chi - dopo anni di problemi seri (endometriosi e fibromi uterino) - sta raggiungendo i 50 anni. (Sono consapevole di essere anzianotta, ma la speranza c'è sempre). Grazie\n",
      "\n",
      "Risultati finali dopo unione dei modelli:\n",
      "Parola: giulia, Entità: PER, Score: 0.9279\n",
      "--------------------------------\n",
      "Parola: milano, Entità: LOC, Score: 0.7912\n",
      "--------------------------------\n",
      "Parola: endometriosi, Entità: DIAGNOSI E COMORBIDITA (B), Score: 0.7861\n",
      "--------------------------------\n",
      "Parola: fibromi, Entità: DIAGNOSI E COMORBIDITA (B), Score: 0.8488\n",
      "--------------------------------\n",
      "Parola: uterino, Entità: DIAGNOSI E COMORBIDITA (I), Score: 0.7146\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Definisci il testo su cui eseguire l'analisi\n",
    "text = \"Vorrei sapere se Giulia che vive Milano c'è qualche (seria)speranza anche per chi - dopo anni di problemi seri (endometriosi e fibromi uterino) - sta raggiungendo i 50 anni. (Sono consapevole di essere anzianotta, ma la speranza c'è sempre). Grazie\"\n",
    "\n",
    "# 4. Esegui l'analisi sui modelli solo se le pipeline sono caricate correttamente\n",
    "result_1 = []\n",
    "if model_1_pipeline:\n",
    "    result_1 = model_1_pipeline(text)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 1: {model_1_id}\")\n",
    "\n",
    "result_2 = []\n",
    "if model_2_pipeline:\n",
    "    result_2 = model_2_pipeline(text)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 2: {model_2_id}\")\n",
    "\n",
    "# 5. Ricostruisci i risultati dai sub-token in parole complete per entrambi i modelli\n",
    "reconstructed_model_1_results = reconstruct_word(result_1) if result_1 else []\n",
    "reconstructed_model_2_results = reconstruct_word(result_2) if result_2 else []\n",
    "\n",
    "# 6. Unisci i risultati dei due modelli, con la preferenza per il modello 2\n",
    "final_results = merge_results(reconstructed_model_1_results, reconstructed_model_2_results)\n",
    "\n",
    "# 7. Stampa i risultati finali in modo leggibile\n",
    "print(text)\n",
    "print(\"\\nRisultati finali dopo unione dei modelli:\")\n",
    "for result in final_results:\n",
    "    print(f\"Parola: {result['word']}, Entità: {result['entity']}, Score: {result['score']:.4f}\")\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faccio una prova puleno il testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorrei sapere se Giulia che vive Milano c'è qualche (seria)speranza anche per chi - dopo anni di problemi seri (endometriosi e fibromi uterino) - sta raggiungendo i 50 anni. (Sono consapevole di essere anzianotta, ma la speranza c'è sempre). Grazie\n",
      "volere sapere Giulia vivere Milano c ' serio speranza problema serio endometrioso fibrome uterino raggiungere il 50 . consapevole anzianotta , speranza c ' .\n",
      "\n",
      "Risultati finali dopo unione dei modelli:\n",
      "Parola: giulia, Entità: MISC, Score: 0.3940\n",
      "--------------------------------\n",
      "Parola: milano, Entità: MISC, Score: 0.5187\n",
      "--------------------------------\n",
      "Parola: speranza, Entità: MISC, Score: 0.3922\n",
      "--------------------------------\n",
      "Parola: endometrioso, Entità: DIAGNOSI E COMORBIDITA (B), Score: 0.4980\n",
      "--------------------------------\n",
      "Parola: fime, Entità: DIAGNOSI E COMORBIDITA (I), Score: 0.5342\n",
      "--------------------------------\n",
      "Parola: uterino, Entità: DIAGNOSI E COMORBIDITA (I), Score: 0.6979\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Definisci il testo su cui eseguire l'analisi\n",
    "text = \"Vorrei sapere se Giulia che vive Milano c'è qualche (seria)speranza anche per chi - dopo anni di problemi seri (endometriosi e fibromi uterino) - sta raggiungendo i 50 anni. (Sono consapevole di essere anzianotta, ma la speranza c'è sempre). Grazie\"\n",
    "\n",
    "text_processed = preprocess_text(text)\n",
    "text_processed = preprocess_stopwords(text_processed)\n",
    "text_processed = preprocess_lemmatization(text_processed)\n",
    "\n",
    "# 4. Esegui l'analisi sui modelli solo se le pipeline sono caricate correttamente\n",
    "result_1 = []\n",
    "if model_1_pipeline:\n",
    "    result_1 = model_1_pipeline(text_processed)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 1: {model_1_id}\")\n",
    "\n",
    "result_2 = []\n",
    "if model_2_pipeline:\n",
    "    result_2 = model_2_pipeline(text_processed)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 2: {model_2_id}\")\n",
    "\n",
    "# 5. Ricostruisci i risultati dai sub-token in parole complete per entrambi i modelli\n",
    "reconstructed_model_1_results = reconstruct_word(result_1) if result_1 else []\n",
    "reconstructed_model_2_results = reconstruct_word(result_2) if result_2 else []\n",
    "\n",
    "# 6. Unisci i risultati dei due modelli, con la preferenza per il modello 2\n",
    "final_results = merge_results(reconstructed_model_1_results, reconstructed_model_2_results)\n",
    "\n",
    "# 7. Stampa i risultati finali in modo leggibile\n",
    "print(text)\n",
    "print(text_processed)\n",
    "print(\"\\nRisultati finali dopo unione dei modelli:\")\n",
    "for result in final_results:\n",
    "    print(f\"Parola: {result['word']}, Entità: {result['entity']}, Score: {result['score']:.4f}\")\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'desire', 'score': 0.44079965353012085}, {'label': 'curiosity', 'score': 0.31969621777534485}, {'label': 'optimism', 'score': 0.2585451006889343}, {'label': 'neutral', 'score': 0.0955871194601059}, {'label': 'confusion', 'score': 0.02322000078856945}, {'label': 'caring', 'score': 0.0206000916659832}, {'label': 'approval', 'score': 0.019204890355467796}, {'label': 'disappointment', 'score': 0.011250891722738743}, {'label': 'sadness', 'score': 0.009548953734338284}, {'label': 'admiration', 'score': 0.006543469615280628}, {'label': 'love', 'score': 0.006385591346770525}, {'label': 'excitement', 'score': 0.006221286486834288}, {'label': 'disapproval', 'score': 0.005997466389089823}, {'label': 'realization', 'score': 0.005887140985578299}, {'label': 'annoyance', 'score': 0.005676315166056156}, {'label': 'surprise', 'score': 0.005030387546867132}, {'label': 'fear', 'score': 0.0038803881034255028}, {'label': 'remorse', 'score': 0.0031880855094641447}, {'label': 'nervousness', 'score': 0.0027924864552915096}, {'label': 'joy', 'score': 0.0021816191729158163}, {'label': 'anger', 'score': 0.0020208931528031826}, {'label': 'amusement', 'score': 0.0019022759515792131}, {'label': 'disgust', 'score': 0.0016637983499094844}, {'label': 'grief', 'score': 0.0011236581485718489}, {'label': 'gratitude', 'score': 0.0009536580764688551}, {'label': 'relief', 'score': 0.0006443101447075605}, {'label': 'embarrassment', 'score': 0.0006158652831800282}, {'label': 'pride', 'score': 0.00034352525835856795}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=model_3_id, top_k=None)\n",
    "\n",
    "sentences = [\"I would like to know if Giulia who lives Milan there is some (serious) hope also for those who - after years of serious problems (endometrosis and uterine fibroids) - are reaching 50 years. (I am aware that I am elderly, but there is always hope)\"]\n",
    "\n",
    "model_outputs = classifier(sentences)\n",
    "print(model_outputs[0])\n",
    "# produces a list of dicts for each of the labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\giuly\\Documents\\GitHub\\data_semantics\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testo tradotto: I would like to know if Giulia who lives Milan there is some (serious) hope also for those who - after years of serious problems (endometrosis and uterine fibroids) - are reaching 50 years. (I am aware that I am elderly, but there is always hope).\n"
     ]
    }
   ],
   "source": [
    "text = \"Vorrei sapere se Giulia che vive Milano c'è qualche (seria)speranza anche per chi - dopo anni di problemi seri (endometriosi e fibromi uterino) - sta raggiungendo i 50 anni. (Sono consapevole di essere anzianotta, ma la speranza c'è sempre). Grazie\"\n",
    "\n",
    "\n",
    "def traduci_testo(testo):\n",
    "    # Carica il modello e il tokenizer per italiano -> inglese\n",
    "    model_name = \"Helsinki-NLP/opus-mt-it-en\"\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenizza il testo\n",
    "    inputs = tokenizer(testo, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Ottieni la traduzione\n",
    "    translated = model.generate(**inputs)\n",
    "    \n",
    "    # Decodifica la traduzione\n",
    "    tradotto = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    return tradotto\n",
    "\n",
    "# Testo da tradurre\n",
    "testo_in_italiano = text\n",
    "traduzione = traduci_testo(testo_in_italiano)\n",
    "\n",
    "# Stampa il risultato\n",
    "print(f\"Testo tradotto: {traduzione}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
