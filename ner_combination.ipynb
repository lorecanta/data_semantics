{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model_and_files, get_model_details, reconstruct_word, merge_results, preprocess_text, preprocess_stopwords, preprocess_lemmatization, traduci_output, process_text_with_models, process_emotions_and_translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### carico i modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento modello 1: osiria/bert-italian-uncased-ner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento modello 2: IVN-RIN/MedPsyNIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento modello 3: SamLowe/roberta-base-go_emotions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento modello 4: Helsinki-NLP/opus-mt-it-en\n"
     ]
    }
   ],
   "source": [
    "# 1. Recupera i dettagli dei modelli\n",
    "model_details = get_model_details()\n",
    "\n",
    "# 2. Assegna manualmente le tuple a  separate\n",
    "(model_1_id, model_1_type, model_1_files), (model_2_id, model_2_type, model_2_files), (model_3_id, model_3_type, model_3_files), (model_4_id, model_4_type, model_4_files) = model_details\n",
    "\n",
    "# 3. Usa i dettagli per caricare i modelli\n",
    "print(f\"Caricamento modello 1: {model_1_id}\")\n",
    "model_1_pipeline = load_model_and_files(model_1_id, model_type=model_1_type)\n",
    "\n",
    "print(f\"Caricamento modello 2: {model_2_id}\")\n",
    "model_2_pipeline = load_model_and_files(model_2_id, model_type=model_2_type)\n",
    "\n",
    "print(f\"Caricamento modello 3: {model_3_id}\")\n",
    "model_3_pipeline = load_model_and_files(model_3_id, model_type=model_3_type)\n",
    "\n",
    "print(f\"Caricamento modello 4: {model_4_id}\")\n",
    "model_4_pipeline = load_model_and_files(model_4_id, model_type=model_4_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Ciao Lorenzo, ho scoperto una cosa che davvero mi ha sconvolto. Ho parlato con Giada a Bologna, e mi ha detto che l'altra sera ti ha visto al bar con un'altra donna. Io davvero sono senza parole, pretendo delle spiegazioni. Mi sono fidata di te, anche dopo che mi hai rassicurato più e più volte sulla tua fedeltà nei miei confronti. Ora basta. Ti voglio fuori dalla mia vita e dalla mia casa. L'hai tanto denigrata, finalmente sei libero di trovarne una tua come piace a te. PS: fottiti.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faccio una prova utilizzando direttamente i modelli sul testo crudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao Lorenzo, ho scoperto una cosa che davvero mi ha sconvolto. Ho parlato con Giada a Bologna, e mi ha detto che l'altra sera ti ha visto al bar con un'altra donna. Io davvero sono senza parole, pretendo delle spiegazioni. Mi sono fidata di te, anche dopo che mi hai rassicurato più e più volte sulla tua fedeltà nei miei confronti. Ora basta. Ti voglio fuori dalla mia vita e dalla mia casa. L'hai tanto denigrata, finalmente sei libero di trovarne una tua come piace a te. PS: fottiti.\n",
      "\n",
      "Risultati finali dopo unione dei modelli:\n",
      "Parola: lorenzo, Entità: PER, Score: 0.9657\n",
      "--------------------------------\n",
      "Parola: giada, Entità: PER, Score: 0.9027\n",
      "--------------------------------\n",
      "Parola: bologna, Entità: LOC, Score: 0.9925\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Esegui l'analisi sui modelli solo se le pipeline sono caricate correttamente\n",
    "result_1 = []\n",
    "if model_1_pipeline:\n",
    "    result_1 = model_1_pipeline(text)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 1: {model_1_id}\")\n",
    "\n",
    "result_2 = []\n",
    "if model_2_pipeline:\n",
    "    result_2 = model_2_pipeline(text)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 2: {model_2_id}\")\n",
    "\n",
    "# 5. Ricostruisci i risultati dai sub-token in parole complete per entrambi i modelli\n",
    "reconstructed_model_1_results = reconstruct_word(result_1) if result_1 else []\n",
    "reconstructed_model_2_results = reconstruct_word(result_2) if result_2 else []\n",
    "\n",
    "# 6. Unisci i risultati dei due modelli, con la preferenza per il modello 2\n",
    "final_results = merge_results(reconstructed_model_1_results, reconstructed_model_2_results)\n",
    "\n",
    "# 7. Stampa i risultati finali in modo leggibile\n",
    "print(text)\n",
    "print(\"\\nRisultati finali dopo unione dei modelli:\")\n",
    "for result in final_results:\n",
    "    print(f\"Parola: {result['word']}, Entità: {result['entity']}, Score: {result['score']:.4f}\")\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faccio una prova puleno il testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorrei sapere se Giulia che vive Milano c'è qualche (seria)speranza anche per chi - dopo anni di problemi seri (endometriosi e fibromi uterino) - sta raggiungendo i 50 anni. (Sono consapevole di essere anzianotta, ma la speranza c'è sempre). Grazie\n",
      "volere sapere Giulia vivere Milano c ' serio speranza problema serio endometrioso fibrome uterino raggiungere il 50 . consapevole anzianotta , speranza c ' .\n",
      "\n",
      "Risultati finali dopo unione dei modelli:\n",
      "Parola: giulia, Entità: MISC, Score: 0.3940\n",
      "--------------------------------\n",
      "Parola: milano, Entità: MISC, Score: 0.5187\n",
      "--------------------------------\n",
      "Parola: speranza, Entità: MISC, Score: 0.3922\n",
      "--------------------------------\n",
      "Parola: endometrioso, Entità: DIAGNOSI E COMORBIDITA (B), Score: 0.4980\n",
      "--------------------------------\n",
      "Parola: fime, Entità: DIAGNOSI E COMORBIDITA (I), Score: 0.5342\n",
      "--------------------------------\n",
      "Parola: uterino, Entità: DIAGNOSI E COMORBIDITA (I), Score: 0.6979\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Definisci il testo su cui eseguire l'analisi\n",
    "\n",
    "text_processed = preprocess_text(text)\n",
    "text_processed = preprocess_stopwords(text_processed)\n",
    "text_processed = preprocess_lemmatization(text_processed)\n",
    "\n",
    "# 4. Esegui l'analisi sui modelli solo se le pipeline sono caricate correttamente\n",
    "result_1 = []\n",
    "if model_1_pipeline:\n",
    "    result_1 = model_1_pipeline(text_processed)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 1: {model_1_id}\")\n",
    "\n",
    "result_2 = []\n",
    "if model_2_pipeline:\n",
    "    result_2 = model_2_pipeline(text_processed)\n",
    "else:\n",
    "    print(f\"Errore nel caricare la pipeline per il modello 2: {model_2_id}\")\n",
    "\n",
    "# 5. Ricostruisci i risultati dai sub-token in parole complete per entrambi i modelli\n",
    "reconstructed_model_1_results = reconstruct_word(result_1) if result_1 else []\n",
    "reconstructed_model_2_results = reconstruct_word(result_2) if result_2 else []\n",
    "\n",
    "# 6. Unisci i risultati dei due modelli, con la preferenza per il modello 2\n",
    "final_results = merge_results(reconstructed_model_1_results, reconstructed_model_2_results)\n",
    "\n",
    "# 7. Stampa i risultati finali in modo leggibile\n",
    "print(text)\n",
    "print(text_processed)\n",
    "print(\"\\nRisultati finali dopo unione dei modelli:\")\n",
    "for result in final_results:\n",
    "    print(f\"Parola: {result['word']}, Entità: {result['entity']}, Score: {result['score']:.4f}\")\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### faccio sentiment Analys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tradurre il testo con il modello di traduzione\n",
    "text_tradotto = model_4_pipeline(text)\n",
    "\n",
    "# Classificare le emozioni nel testo tradotto\n",
    "output_classificazione = model_3_pipeline(text_tradotto)\n",
    "\n",
    "# Tradurre i label delle emozioni in italiano\n",
    "output_tradotto = traduci_output(output_classificazione[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'gioia', 'score': 0.5552638173103333},\n",
       " {'label': 'ammirazione', 'score': 0.28733378648757935},\n",
       " {'label': 'divertimento', 'score': 0.1629708707332611},\n",
       " {'label': 'orgoglio', 'score': 0.13074733316898346},\n",
       " {'label': 'eccitazione', 'score': 0.05566107854247093},\n",
       " {'label': 'approvazione', 'score': 0.048677101731300354},\n",
       " {'label': 'gratitudine', 'score': 0.042775001376867294},\n",
       " {'label': 'rilievo', 'score': 0.03504181653261185},\n",
       " {'label': 'neutrale', 'score': 0.029221637174487114},\n",
       " {'label': 'realizzazione', 'score': 0.02292788401246071},\n",
       " {'label': 'fastidio', 'score': 0.01711045578122139},\n",
       " {'label': 'ottimismo', 'score': 0.013466647826135159},\n",
       " {'label': 'imbarazzo', 'score': 0.00791418831795454},\n",
       " {'label': 'cura', 'score': 0.006322175730019808},\n",
       " {'label': 'sorpresa', 'score': 0.005588737316429615},\n",
       " {'label': 'disapprovazione', 'score': 0.005490300711244345},\n",
       " {'label': 'desiderio', 'score': 0.005143625661730766},\n",
       " {'label': 'delusione', 'score': 0.004559988621622324},\n",
       " {'label': 'rabbia', 'score': 0.004203183576464653},\n",
       " {'label': 'amore', 'score': 0.0038713247049599886},\n",
       " {'label': 'disgusto', 'score': 0.00258399429731071},\n",
       " {'label': 'lutto', 'score': 0.0022922197822481394},\n",
       " {'label': 'tristezza', 'score': 0.002289911499246955},\n",
       " {'label': 'rimorso', 'score': 0.0018747305730357766},\n",
       " {'label': 'paura', 'score': 0.001808751025237143},\n",
       " {'label': 'nervosismo', 'score': 0.001638445071876049},\n",
       " {'label': 'confusione', 'score': 0.001284735626541078},\n",
       " {'label': 'curiosità', 'score': 0.0006226054974831641}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tradotto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
